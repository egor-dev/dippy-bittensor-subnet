
# LLM Scoring Criteria

The general goal of the subnet is to incentivize the development of a roleplay model that can reliably serve millions of users worldwide. 
To accomplish this goal, the subnet aims to adjust and tune the scoring criteria as needed.

## Background
The methods of evaluating model quality have evolved quickly and drastically over time.
Initially, the subnet prioritized dataset based evaluation approaches, where model generated text was scored via accuracy to an existing roleplay dataset. 
While this proved effective for smaller models, the process of evaluating a roleplay model has quickly scaled in terms of computational requirements. 
To better manage this scale, the usage of LLMs as a judge has become more popular as a method of benchmarking and comparing model outputs.

## Dataset
The dippy team has developed a continuously generated dataset based on billions of conversations from the Dippy app. 
While other benchmarks rely on a static dataset that can be overfitted, the dippy dataset will always increase in size to combat this.
This dataset is always being improved upon in terms of both quality and conversation length.
With over 1 million conversations, the dippy dataset serves as the basis for model scoring.
The dippy dataset is developed at full cost to the dippy team. 

## Current Scoring Mechanism

### Judge Score
For scoring, `gpt-4o` is used as a judge to compare conversations based on the original dippy dataset versus conversations generated by a miner submitted model.
The current criteria compares the following:

```
- realism: The responses match the character's personality perfectly, staying true to who they are meant to be without any inconsistencies
- entertainment: The responses are engaging and interesting to read, without any repetitive or boring language
- coherency: The responses use clear and correct language throughout, with proper grammar, spelling and punctuation. The writing style stays consistent and appropriate.
```

If according to the judge, the model generated text is considered qualitatively better than the original dataset text, that counts as a "win" against the dataset.
The number of wins against the total number of evaluations comprises the judge score.

### Post Evaluation 
After initial evaluation, a model will be selected for post evaluation after some time. 
The current process for this is a proprietary solution that is based on judging criteria from SOTA model benchmarking approaches. 
In the future, the details for this will be available on https://research.dippy.ai
